{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### NLP - Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words (BoW) model in NLP\n",
    "https://www.geeksforgeeks.org/bag-of-words-bow-model-in-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "#                the world have come and invaded us, captured our lands, conquered our minds. \n",
    "#                From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "#                the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "#                Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "#                We have not grabbed their land, their culture, \n",
    "#                their history and tried to enforce our way of life on them. \n",
    "#                Why? Because we respect the freedom of others. That is why my \n",
    "#                first vision is that of freedom. I believe that India got its first vision of \n",
    "#                this in 1857, when we started the War of Independence. It is this freedom that\n",
    "#                we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "#                My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "#                It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "#                in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "#                Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "#                see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "#                I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "#                stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "#                strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "#                My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "#                space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "#                I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "#                I see four milestones in my career.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python3 code for preprocessing text \n",
    "# import nltk \n",
    "# import re \n",
    "# import numpy as np \n",
    "\n",
    "# # execute the text here as : \n",
    "# # text = \"\"\" # place text here \"\"\" \n",
    "# dataset = nltk.sent_tokenize(paragraph) \n",
    "# for i in range(len(dataset)): \n",
    "# \tdataset[i] = dataset[i].lower() \n",
    "# \tdataset[i] = re.sub(r'\\W', ' ', dataset[i]) \n",
    "# \tdataset[i] = re.sub(r'\\s+', ' ', dataset[i]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# import numpy as np\n",
    "\n",
    "# # Download required NLTK data files\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# # Sample paragraph\n",
    "# paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "#                the world have come and invaded us, captured our lands, conquered our minds. \n",
    "#                From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "#                the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "#                Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "#                We have not grabbed their land, their culture, \n",
    "#                their history and tried to enforce our way of life on them. \n",
    "#                Why? Because we respect the freedom of others. That is why my \n",
    "#                first vision is that of freedom. I believe that India got its first vision of \n",
    "#                this in 1857, when we started the War of Independence. It is this freedom that\n",
    "#                we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "#                My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "#                It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "#                in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "#                Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "#                see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "#                I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "#                stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "#                strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "#                My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "#                space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "#                I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "#                I see four milestones in my career.\"\"\"\n",
    "\n",
    "# # Step 1: Tokenize the paragraph into sentences\n",
    "# tokenized_sentences = sent_tokenize(paragraph)\n",
    "\n",
    "# # Step 2: Define stop words and preprocess each sentence\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# cleaned_sentences = []\n",
    "\n",
    "# for sentence in tokenized_sentences:\n",
    "#     words = word_tokenize(sentence.lower())  # Tokenize into words and convert to lowercase\n",
    "#     filtered_words = [word for word in words if word.isalnum() and word not in stop_words]  # Remove stopwords and non-alphanumeric words\n",
    "#     cleaned_sentences.append(filtered_words)\n",
    "\n",
    "# # Step 3: Create a unique vocabulary from all the words\n",
    "# vocabulary = list(set(word for sentence in cleaned_sentences for word in sentence))\n",
    "\n",
    "# # Step 4: Generate the Bag of Words matrix\n",
    "# bow_matrix = np.zeros((len(cleaned_sentences), len(vocabulary)), dtype=int)\n",
    "\n",
    "# for i, sentence in enumerate(cleaned_sentences):\n",
    "#     for word in sentence:\n",
    "#         if word in vocabulary:\n",
    "#             bow_matrix[i, vocabulary.index(word)] += 1\n",
    "\n",
    "# # Step 5: Print the results\n",
    "# print(\"Vocabulary:\\n\", vocabulary)\n",
    "# print(\"\\nBag of Words Matrix:\\n\", bow_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 5: Plot the Bag of Words matrix\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(bow_matrix, annot=False, cmap='viridis', cbar=True, xticklabels=vocabulary, yticklabels=range(1, len(tokenized_sentences) + 1))\n",
    "# plt.title(\"Bag of Words Matrix Heatmap\")\n",
    "# plt.xlabel(\"Words\")\n",
    "# plt.ylabel(\"Sentences\")\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df_bow = pd.DataFrame(bow_matrix, columns=vocabulary, index=[f\"Sentence {i+1}\" for i in range(len(cleaned_sentences))])\n",
    "\n",
    "# # Step 6: Display the Bag of Words matrix as a table\n",
    "# # print(\"Bag of Words Matrix (Table Format):\")\n",
    "# print(df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Bag of Words(NLP) Using sklearn CountVectorizer\n",
    "https://medium.com/analytics-vidhya/implementation-of-bag-of-words-nlp-397f4cf67970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Bag of words (BOWs)?\n",
    "Bag of words is a way of representing text data in NLP, when modeling text with machine learning algorithm. It is a simple method and very flexible to use in modeling.\n",
    "\n",
    "In general, Bag of words used to convert words in a text into a matrix representation by extracting its features i.e., it shows us which word occurs in a sentence and its frequency, for use in modeling such as machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/koushikdev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"sky is nice. clouds are nice. Sky is nice and Clouds are nice.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 => tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentences=sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sky is nice.', 'clouds are nice.', 'Sky is nice and Clouds are nice.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Remove periods using regex\n",
    "cleaned_sentences = [re.sub(r'\\.', '', sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sky is nice', 'clouds are nice', 'Sky is nice and Clouds are nice']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence--->words tokenize and removing stop words\n",
    "cleaned_sentence = []\n",
    "\n",
    "for sentence in cleaned_sentences:\n",
    "    word = sentence.lower()  \n",
    "    word = word.split()    ##splitting our sentence into words \n",
    "    \n",
    "    ##removing stop words\n",
    "    word = [i for i in word if i not in set(stopwords.words('english'))]          \n",
    "    word = \" \".join(word)   \n",
    "    cleaned_sentence.append(word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sky nice', 'clouds nice', 'sky nice clouds nice']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "##importing Bag-of-words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# cv = CountVectorizer(max_features = 3)  ##give it a max features as 3\n",
    "# Bagofwords = cv.fit_transform(cleaned_sentence).toarray()\n",
    "\n",
    "# An instance of CountVectorizer is created. By default, it converts all characters to lowercase and removes punctuation\n",
    "'''\n",
    "Using max_features=3 in CountVectorizer limits the number of unique words (features) \n",
    "to the top 3 most frequently occurring words in the corpus. \n",
    "This parameter is helpful when you want to control the size of your vocabulary, especially \n",
    "when dealing with a large dataset or when you want to focus on the most relevant features for your analysis or model.\n",
    "'''\n",
    "\n",
    "# An instance of CountVectorizer is created. By default, it converts all characters to lowercase and removes punctuation.\n",
    "vectorizer =CountVectorizer(max_features = 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fit: Learns the vocabulary from the corpus (i.e., all unique words found across the documents).\n",
    "Transform: Converts the documents into a matrix of token counts.\n",
    "X is a sparse matrix representing the word counts in each document.\n",
    "''' \n",
    "X = vectorizer.fit_transform(cleaned_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['clouds' 'nice' 'sky']\n"
     ]
    }
   ],
   "source": [
    "Vocabulary = vectorizer.get_feature_names_out()\n",
    "print(\"Vocabulary:\",Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1],\n",
       "       [1, 1, 0],\n",
       "       [1, 2, 1]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bagofwords = X.toarray()\n",
    "Bagofwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()\n",
    "print(X.toarray())\n",
    "\n",
    "# vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "# X2 = vectorizer2.fit_transform(corpus)\n",
    "# vectorizer2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
