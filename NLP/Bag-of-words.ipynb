{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### NLP - Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words (BoW) model in NLP\n",
    "https://www.geeksforgeeks.org/bag-of-words-bow-model-in-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "#                the world have come and invaded us, captured our lands, conquered our minds. \n",
    "#                From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "#                the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "#                Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "#                We have not grabbed their land, their culture, \n",
    "#                their history and tried to enforce our way of life on them. \n",
    "#                Why? Because we respect the freedom of others. That is why my \n",
    "#                first vision is that of freedom. I believe that India got its first vision of \n",
    "#                this in 1857, when we started the War of Independence. It is this freedom that\n",
    "#                we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "#                My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "#                It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "#                in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "#                Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "#                see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "#                I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "#                stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "#                strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "#                My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "#                space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "#                I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "#                I see four milestones in my career.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Bag of Words(NLP) Using sklearn CountVectorizer\n",
    "https://medium.com/analytics-vidhya/implementation-of-bag-of-words-nlp-397f4cf67970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Bag of words (BOWs)?\n",
    "Bag of words is a way of representing text data in NLP, when modeling text with machine learning algorithm. It is a simple method and very flexible to use in modeling.\n",
    "\n",
    "In general, Bag of words used to convert words in a text into a matrix representation by extracting its features i.e., it shows us which word occurs in a sentence and its frequency, for use in modeling such as machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/koushikdev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"sky is nice. clouds are nice. Sky is nice and Clouds are nice.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 => tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentences=sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sky is nice.', 'clouds are nice.', 'Sky is nice and Clouds are nice.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Remove periods using regex\n",
    "cleaned_sentences = [re.sub(r'\\.', '', sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sky is nice', 'clouds are nice', 'Sky is nice and Clouds are nice']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence--->words tokenize and removing stop words\n",
    "cleaned_sentence = []\n",
    "\n",
    "for sentence in cleaned_sentences:\n",
    "    word = sentence.lower()  \n",
    "    word = word.split()    ##splitting our sentence into words \n",
    "    \n",
    "    ##removing stop words\n",
    "    word = [i for i in word if i not in set(stopwords.words('english'))]          \n",
    "    word = \" \".join(word)   \n",
    "    cleaned_sentence.append(word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sky nice', 'clouds nice', 'sky nice clouds nice']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sky nice', 'clouds nice', 'sky nice clouds nice']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = cleaned_sentence\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "##importing Bag-of-words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# cv = CountVectorizer(max_features = 3)  ##give it a max features as 3\n",
    "# Bagofwords = cv.fit_transform(cleaned_sentence).toarray()\n",
    "\n",
    "# An instance of CountVectorizer is created. By default, it converts all characters to lowercase and removes punctuation\n",
    "'''\n",
    "Using max_features=3 in CountVectorizer limits the number of unique words (features) \n",
    "to the top 3 most frequently occurring words in the corpus. \n",
    "This parameter is helpful when you want to control the size of your vocabulary, especially \n",
    "when dealing with a large dataset or when you want to focus on the most relevant features for your analysis or model.\n",
    "'''\n",
    "\n",
    "# An instance of CountVectorizer is created. By default, it converts all characters to lowercase and removes punctuation.\n",
    "vectorizer = CountVectorizer(max_features = 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fit: Learns the vocabulary from the corpus (i.e., all unique words found across the documents).\n",
    "Transform: Converts the documents into a matrix of token counts.\n",
    "X is a sparse matrix representing the word counts in each document.\n",
    "''' \n",
    "X = vectorizer.fit_transform(corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['clouds' 'nice' 'sky']\n"
     ]
    }
   ],
   "source": [
    "Vocabulary = vectorizer.get_feature_names_out()\n",
    "print(\"Vocabulary:\",Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bagofwords = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1],\n",
       "       [1, 1, 0],\n",
       "       [1, 2, 1]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bagofwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Index Type  Size                                              Value\n",
      "0      0  str     6                    i have three visions for india \n",
      "1      1  str    23  in 3000 years of our history people from all o...\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to create and style a DataFrame\n",
    "def create_data_frame(dataset):\n",
    "    data = {\n",
    "        'Index': range(len(dataset)),\n",
    "        'Type': ['str' for _ in dataset],\n",
    "        'Size': [len(sentence.split()) for sentence in dataset],\n",
    "        'Value': dataset\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Style the DataFrame\n",
    "    styled_df = df.style.set_table_attributes('style=\"width:100%; border-collapse: collapse;\"').set_properties(**{'border': '1px solid black', 'padding': '5px'})\n",
    "    \n",
    "    return styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a5d61_row0_col0, #T_a5d61_row0_col1, #T_a5d61_row0_col2, #T_a5d61_row0_col3, #T_a5d61_row1_col0, #T_a5d61_row1_col1, #T_a5d61_row1_col2, #T_a5d61_row1_col3, #T_a5d61_row2_col0, #T_a5d61_row2_col1, #T_a5d61_row2_col2, #T_a5d61_row2_col3, #T_a5d61_row3_col0, #T_a5d61_row3_col1, #T_a5d61_row3_col2, #T_a5d61_row3_col3, #T_a5d61_row4_col0, #T_a5d61_row4_col1, #T_a5d61_row4_col2, #T_a5d61_row4_col3, #T_a5d61_row5_col0, #T_a5d61_row5_col1, #T_a5d61_row5_col2, #T_a5d61_row5_col3, #T_a5d61_row6_col0, #T_a5d61_row6_col1, #T_a5d61_row6_col2, #T_a5d61_row6_col3, #T_a5d61_row7_col0, #T_a5d61_row7_col1, #T_a5d61_row7_col2, #T_a5d61_row7_col3, #T_a5d61_row8_col0, #T_a5d61_row8_col1, #T_a5d61_row8_col2, #T_a5d61_row8_col3, #T_a5d61_row9_col0, #T_a5d61_row9_col1, #T_a5d61_row9_col2, #T_a5d61_row9_col3, #T_a5d61_row10_col0, #T_a5d61_row10_col1, #T_a5d61_row10_col2, #T_a5d61_row10_col3, #T_a5d61_row11_col0, #T_a5d61_row11_col1, #T_a5d61_row11_col2, #T_a5d61_row11_col3, #T_a5d61_row12_col0, #T_a5d61_row12_col1, #T_a5d61_row12_col2, #T_a5d61_row12_col3, #T_a5d61_row13_col0, #T_a5d61_row13_col1, #T_a5d61_row13_col2, #T_a5d61_row13_col3, #T_a5d61_row14_col0, #T_a5d61_row14_col1, #T_a5d61_row14_col2, #T_a5d61_row14_col3, #T_a5d61_row15_col0, #T_a5d61_row15_col1, #T_a5d61_row15_col2, #T_a5d61_row15_col3, #T_a5d61_row16_col0, #T_a5d61_row16_col1, #T_a5d61_row16_col2, #T_a5d61_row16_col3, #T_a5d61_row17_col0, #T_a5d61_row17_col1, #T_a5d61_row17_col2, #T_a5d61_row17_col3, #T_a5d61_row18_col0, #T_a5d61_row18_col1, #T_a5d61_row18_col2, #T_a5d61_row18_col3, #T_a5d61_row19_col0, #T_a5d61_row19_col1, #T_a5d61_row19_col2, #T_a5d61_row19_col3, #T_a5d61_row20_col0, #T_a5d61_row20_col1, #T_a5d61_row20_col2, #T_a5d61_row20_col3, #T_a5d61_row21_col0, #T_a5d61_row21_col1, #T_a5d61_row21_col2, #T_a5d61_row21_col3, #T_a5d61_row22_col0, #T_a5d61_row22_col1, #T_a5d61_row22_col2, #T_a5d61_row22_col3, #T_a5d61_row23_col0, #T_a5d61_row23_col1, #T_a5d61_row23_col2, #T_a5d61_row23_col3, #T_a5d61_row24_col0, #T_a5d61_row24_col1, #T_a5d61_row24_col2, #T_a5d61_row24_col3, #T_a5d61_row25_col0, #T_a5d61_row25_col1, #T_a5d61_row25_col2, #T_a5d61_row25_col3, #T_a5d61_row26_col0, #T_a5d61_row26_col1, #T_a5d61_row26_col2, #T_a5d61_row26_col3, #T_a5d61_row27_col0, #T_a5d61_row27_col1, #T_a5d61_row27_col2, #T_a5d61_row27_col3, #T_a5d61_row28_col0, #T_a5d61_row28_col1, #T_a5d61_row28_col2, #T_a5d61_row28_col3, #T_a5d61_row29_col0, #T_a5d61_row29_col1, #T_a5d61_row29_col2, #T_a5d61_row29_col3, #T_a5d61_row30_col0, #T_a5d61_row30_col1, #T_a5d61_row30_col2, #T_a5d61_row30_col3, #T_a5d61_row31_col0, #T_a5d61_row31_col1, #T_a5d61_row31_col2, #T_a5d61_row31_col3 {\n",
       "  border: 1px solid black;\n",
       "  padding: 5px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a5d61\" style=\"width:100%; border-collapse: collapse;\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a5d61_level0_col0\" class=\"col_heading level0 col0\" >Index</th>\n",
       "      <th id=\"T_a5d61_level0_col1\" class=\"col_heading level0 col1\" >Type</th>\n",
       "      <th id=\"T_a5d61_level0_col2\" class=\"col_heading level0 col2\" >Size</th>\n",
       "      <th id=\"T_a5d61_level0_col3\" class=\"col_heading level0 col3\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a5d61_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_a5d61_row0_col1\" class=\"data row0 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row0_col2\" class=\"data row0 col2\" >6</td>\n",
       "      <td id=\"T_a5d61_row0_col3\" class=\"data row0 col3\" >i have three visions for india </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a5d61_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_a5d61_row1_col1\" class=\"data row1 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row1_col2\" class=\"data row1 col2\" >23</td>\n",
       "      <td id=\"T_a5d61_row1_col3\" class=\"data row1 col3\" >in 3000 years of our history people from all over the world have come and invaded us captured our lands conquered our minds </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_a5d61_row2_col0\" class=\"data row2 col0\" >2</td>\n",
       "      <td id=\"T_a5d61_row2_col1\" class=\"data row2 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row2_col2\" class=\"data row2 col2\" >29</td>\n",
       "      <td id=\"T_a5d61_row2_col3\" class=\"data row2 col3\" > from alexander onwards the greeks the turks the moguls the portuguese the british the french the dutch all of them came and looted us took over what was ours </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_a5d61_row3_col0\" class=\"data row3 col0\" >3</td>\n",
       "      <td id=\"T_a5d61_row3_col1\" class=\"data row3 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row3_col2\" class=\"data row3 col2\" >10</td>\n",
       "      <td id=\"T_a5d61_row3_col3\" class=\"data row3 col3\" > yet we have not done this to any other nation </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_a5d61_row4_col0\" class=\"data row4 col0\" >4</td>\n",
       "      <td id=\"T_a5d61_row4_col1\" class=\"data row4 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row4_col2\" class=\"data row4 col2\" >5</td>\n",
       "      <td id=\"T_a5d61_row4_col3\" class=\"data row4 col3\" >we have not conquered anyone </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_a5d61_row5_col0\" class=\"data row5 col0\" >5</td>\n",
       "      <td id=\"T_a5d61_row5_col1\" class=\"data row5 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row5_col2\" class=\"data row5 col2\" >20</td>\n",
       "      <td id=\"T_a5d61_row5_col3\" class=\"data row5 col3\" > we have not grabbed their land their culture their history and tried to enforce our way of life on them </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_a5d61_row6_col0\" class=\"data row6 col0\" >6</td>\n",
       "      <td id=\"T_a5d61_row6_col1\" class=\"data row6 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row6_col2\" class=\"data row6 col2\" >1</td>\n",
       "      <td id=\"T_a5d61_row6_col3\" class=\"data row6 col3\" > why </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_a5d61_row7_col0\" class=\"data row7 col0\" >7</td>\n",
       "      <td id=\"T_a5d61_row7_col1\" class=\"data row7 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row7_col2\" class=\"data row7 col2\" >7</td>\n",
       "      <td id=\"T_a5d61_row7_col3\" class=\"data row7 col3\" >because we respect the freedom of others </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_a5d61_row8_col0\" class=\"data row8 col0\" >8</td>\n",
       "      <td id=\"T_a5d61_row8_col1\" class=\"data row8 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row8_col2\" class=\"data row8 col2\" >10</td>\n",
       "      <td id=\"T_a5d61_row8_col3\" class=\"data row8 col3\" >that is why my first vision is that of freedom </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_a5d61_row9_col0\" class=\"data row9 col0\" >9</td>\n",
       "      <td id=\"T_a5d61_row9_col1\" class=\"data row9 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row9_col2\" class=\"data row9 col2\" >19</td>\n",
       "      <td id=\"T_a5d61_row9_col3\" class=\"data row9 col3\" >i believe that india got its first vision of this in 1857 when we started the war of independence </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_a5d61_row10_col0\" class=\"data row10 col0\" >10</td>\n",
       "      <td id=\"T_a5d61_row10_col1\" class=\"data row10 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row10_col2\" class=\"data row10 col2\" >13</td>\n",
       "      <td id=\"T_a5d61_row10_col3\" class=\"data row10 col3\" >it is this freedom that we must protect and nurture and build on </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_a5d61_row11_col0\" class=\"data row11 col0\" >11</td>\n",
       "      <td id=\"T_a5d61_row11_col1\" class=\"data row11 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row11_col2\" class=\"data row11 col2\" >10</td>\n",
       "      <td id=\"T_a5d61_row11_col3\" class=\"data row11 col3\" >if we are not free no one will respect us </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_a5d61_row12_col0\" class=\"data row12 col0\" >12</td>\n",
       "      <td id=\"T_a5d61_row12_col1\" class=\"data row12 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row12_col2\" class=\"data row12 col2\" >7</td>\n",
       "      <td id=\"T_a5d61_row12_col3\" class=\"data row12 col3\" > my second vision for india s development </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_a5d61_row13_col0\" class=\"data row13 col0\" >13</td>\n",
       "      <td id=\"T_a5d61_row13_col1\" class=\"data row13 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row13_col2\" class=\"data row13 col2\" >9</td>\n",
       "      <td id=\"T_a5d61_row13_col3\" class=\"data row13 col3\" >for fifty years we have been a developing nation </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_a5d61_row14_col0\" class=\"data row14 col0\" >14</td>\n",
       "      <td id=\"T_a5d61_row14_col1\" class=\"data row14 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row14_col2\" class=\"data row14 col2\" >10</td>\n",
       "      <td id=\"T_a5d61_row14_col3\" class=\"data row14 col3\" > it is time we see ourselves as a developed nation </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_a5d61_row15_col0\" class=\"data row15 col0\" >15</td>\n",
       "      <td id=\"T_a5d61_row15_col1\" class=\"data row15 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row15_col2\" class=\"data row15 col2\" >14</td>\n",
       "      <td id=\"T_a5d61_row15_col3\" class=\"data row15 col3\" >we are among the top 5 nations of the world in terms of gdp </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_a5d61_row16_col0\" class=\"data row16 col0\" >16</td>\n",
       "      <td id=\"T_a5d61_row16_col1\" class=\"data row16 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row16_col2\" class=\"data row16 col2\" >10</td>\n",
       "      <td id=\"T_a5d61_row16_col3\" class=\"data row16 col3\" >we have a 10 percent growth rate in most areas </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_a5d61_row17_col0\" class=\"data row17 col0\" >17</td>\n",
       "      <td id=\"T_a5d61_row17_col1\" class=\"data row17 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row17_col2\" class=\"data row17 col2\" >5</td>\n",
       "      <td id=\"T_a5d61_row17_col3\" class=\"data row17 col3\" >our poverty levels are falling </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_a5d61_row18_col0\" class=\"data row18 col0\" >18</td>\n",
       "      <td id=\"T_a5d61_row18_col1\" class=\"data row18 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row18_col2\" class=\"data row18 col2\" >7</td>\n",
       "      <td id=\"T_a5d61_row18_col3\" class=\"data row18 col3\" > our achievements are being globally recognised today </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_a5d61_row19_col0\" class=\"data row19 col0\" >19</td>\n",
       "      <td id=\"T_a5d61_row19_col1\" class=\"data row19 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row19_col2\" class=\"data row19 col2\" >18</td>\n",
       "      <td id=\"T_a5d61_row19_col3\" class=\"data row19 col3\" >yet we lack the self confidence to see ourselves as a developed nation self reliant and self assured </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_a5d61_row20_col0\" class=\"data row20 col0\" >20</td>\n",
       "      <td id=\"T_a5d61_row20_col1\" class=\"data row20 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row20_col2\" class=\"data row20 col2\" >4</td>\n",
       "      <td id=\"T_a5d61_row20_col3\" class=\"data row20 col3\" >isn t this incorrect </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_a5d61_row21_col0\" class=\"data row21 col0\" >21</td>\n",
       "      <td id=\"T_a5d61_row21_col1\" class=\"data row21 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row21_col2\" class=\"data row21 col2\" >5</td>\n",
       "      <td id=\"T_a5d61_row21_col3\" class=\"data row21 col3\" > i have a third vision </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_a5d61_row22_col0\" class=\"data row22 col0\" >22</td>\n",
       "      <td id=\"T_a5d61_row22_col1\" class=\"data row22 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row22_col2\" class=\"data row22 col2\" >7</td>\n",
       "      <td id=\"T_a5d61_row22_col3\" class=\"data row22 col3\" >india must stand up to the world </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_a5d61_row23_col0\" class=\"data row23 col0\" >23</td>\n",
       "      <td id=\"T_a5d61_row23_col1\" class=\"data row23 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row23_col2\" class=\"data row23 col2\" >16</td>\n",
       "      <td id=\"T_a5d61_row23_col3\" class=\"data row23 col3\" >because i believe that unless india stands up to the world no one will respect us </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_a5d61_row24_col0\" class=\"data row24 col0\" >24</td>\n",
       "      <td id=\"T_a5d61_row24_col1\" class=\"data row24 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row24_col2\" class=\"data row24 col2\" >4</td>\n",
       "      <td id=\"T_a5d61_row24_col3\" class=\"data row24 col3\" >only strength respects strength </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_a5d61_row25_col0\" class=\"data row25 col0\" >25</td>\n",
       "      <td id=\"T_a5d61_row25_col1\" class=\"data row25 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row25_col2\" class=\"data row25 col2\" >16</td>\n",
       "      <td id=\"T_a5d61_row25_col3\" class=\"data row25 col3\" >we must be strong not only as a military power but also as an economic power </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_a5d61_row26_col0\" class=\"data row26 col0\" >26</td>\n",
       "      <td id=\"T_a5d61_row26_col1\" class=\"data row26 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row26_col2\" class=\"data row26 col2\" >6</td>\n",
       "      <td id=\"T_a5d61_row26_col3\" class=\"data row26 col3\" >both must go hand in hand </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_a5d61_row27_col0\" class=\"data row27 col0\" >27</td>\n",
       "      <td id=\"T_a5d61_row27_col1\" class=\"data row27 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row27_col2\" class=\"data row27 col2\" >11</td>\n",
       "      <td id=\"T_a5d61_row27_col3\" class=\"data row27 col3\" > my good fortune was to have worked with three great minds </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_a5d61_row28_col0\" class=\"data row28 col0\" >28</td>\n",
       "      <td id=\"T_a5d61_row28_col1\" class=\"data row28 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row28_col2\" class=\"data row28 col2\" >6</td>\n",
       "      <td id=\"T_a5d61_row28_col3\" class=\"data row28 col3\" >dr vikram sarabhai of the dept </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_a5d61_row29_col0\" class=\"data row29 col0\" >29</td>\n",
       "      <td id=\"T_a5d61_row29_col1\" class=\"data row29 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row29_col2\" class=\"data row29 col2\" >16</td>\n",
       "      <td id=\"T_a5d61_row29_col3\" class=\"data row29 col3\" >of space professor satish dhawan who succeeded him and dr brahm prakash father of nuclear material </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_a5d61_row30_col0\" class=\"data row30 col0\" >30</td>\n",
       "      <td id=\"T_a5d61_row30_col1\" class=\"data row30 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row30_col2\" class=\"data row30 col2\" >21</td>\n",
       "      <td id=\"T_a5d61_row30_col3\" class=\"data row30 col3\" > i was lucky to have worked with all three of them closely and consider this the great opportunity of my life </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d61_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_a5d61_row31_col0\" class=\"data row31 col0\" >31</td>\n",
       "      <td id=\"T_a5d61_row31_col1\" class=\"data row31 col1\" >str</td>\n",
       "      <td id=\"T_a5d61_row31_col2\" class=\"data row31 col2\" >7</td>\n",
       "      <td id=\"T_a5d61_row31_col3\" class=\"data row31 col3\" > i see four milestones in my career </td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f9659048460>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "#                the world have come and invaded us, captured our lands, conquered our minds. \n",
    "#                From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "#                the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "#                Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "#                We have not grabbed their land, their culture, \n",
    "#                their history and tried to enforce our way of life on them. \n",
    "#                Why? Because we respect the freedom of others. That is why my \n",
    "#                first vision is that of freedom. I believe that India got its first vision of \n",
    "#                this in 1857, when we started the War of Independence. It is this freedom that\n",
    "#                we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "#                My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "#                It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "#                in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "#                Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "#                see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "#                I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "#                stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "#                strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "#                My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "#                space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "#                I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "#                I see four milestones in my career.\"\"\"\n",
    "\n",
    "# Tokenize and preprocess\n",
    "dataset = nltk.sent_tokenize(text)\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])\n",
    "\n",
    "df = create_data_frame(dataset)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV\n",
    "# df.to_csv('preprocessed_sentences.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''\n",
    "Beans. I was trying to explain to somebody as\n",
    "we were flying in, that’s corn.  That’s beans. And they were very impressed at my \n",
    "agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction.\n",
    "I have a bunch of good friends here today, including somebody who I served with, \n",
    "who is one of the finest senators in the country, and we’re lucky to have him, your Senator,\n",
    "Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven’t seen \n",
    "in a long time, and somehow he has not aged and I have. And it’s great to see you, Governor. \n",
    "I want to thank President Killeen and everybody at the U of I System for making it possible \n",
    "for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. \n",
    "He is somebody who set the path for so much outstanding public service here in Illinois. \n",
    "Now, I want to start by addressing the elephant in the room. I know people are still wondering why \n",
    "I didn’t speak at the commencement.\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re \n",
    "import numpy as np \n",
    "\n",
    "# Convert text to lower case.\n",
    "# Remove all non-word characters.\n",
    "# Remove all punctuations.\n",
    "\n",
    "dataset = nltk.sent_tokenize(corpus) \n",
    "for i in range(len(dataset)): \n",
    "\tdataset[i] = dataset[i].lower() \n",
    "\tdataset[i] = re.sub(r'\\W', ' ', dataset[i]) \n",
    "\tdataset[i] = re.sub(r'\\s+', ' ', dataset[i]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_be237_row0_col0, #T_be237_row0_col1, #T_be237_row0_col2, #T_be237_row0_col3, #T_be237_row1_col0, #T_be237_row1_col1, #T_be237_row1_col2, #T_be237_row1_col3, #T_be237_row2_col0, #T_be237_row2_col1, #T_be237_row2_col2, #T_be237_row2_col3, #T_be237_row3_col0, #T_be237_row3_col1, #T_be237_row3_col2, #T_be237_row3_col3, #T_be237_row4_col0, #T_be237_row4_col1, #T_be237_row4_col2, #T_be237_row4_col3, #T_be237_row5_col0, #T_be237_row5_col1, #T_be237_row5_col2, #T_be237_row5_col3, #T_be237_row6_col0, #T_be237_row6_col1, #T_be237_row6_col2, #T_be237_row6_col3, #T_be237_row7_col0, #T_be237_row7_col1, #T_be237_row7_col2, #T_be237_row7_col3, #T_be237_row8_col0, #T_be237_row8_col1, #T_be237_row8_col2, #T_be237_row8_col3, #T_be237_row9_col0, #T_be237_row9_col1, #T_be237_row9_col2, #T_be237_row9_col3, #T_be237_row10_col0, #T_be237_row10_col1, #T_be237_row10_col2, #T_be237_row10_col3, #T_be237_row11_col0, #T_be237_row11_col1, #T_be237_row11_col2, #T_be237_row11_col3, #T_be237_row12_col0, #T_be237_row12_col1, #T_be237_row12_col2, #T_be237_row12_col3 {\n",
       "  border: 1px solid black;\n",
       "  padding: 5px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_be237\" style=\"width:100%; border-collapse: collapse;\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_be237_level0_col0\" class=\"col_heading level0 col0\" >Index</th>\n",
       "      <th id=\"T_be237_level0_col1\" class=\"col_heading level0 col1\" >Type</th>\n",
       "      <th id=\"T_be237_level0_col2\" class=\"col_heading level0 col2\" >Size</th>\n",
       "      <th id=\"T_be237_level0_col3\" class=\"col_heading level0 col3\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_be237_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_be237_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_be237_row0_col1\" class=\"data row0 col1\" >str</td>\n",
       "      <td id=\"T_be237_row0_col2\" class=\"data row0 col2\" >1</td>\n",
       "      <td id=\"T_be237_row0_col3\" class=\"data row0 col3\" > beans </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be237_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_be237_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_be237_row1_col1\" class=\"data row1 col1\" >str</td>\n",
       "      <td id=\"T_be237_row1_col2\" class=\"data row1 col2\" >15</td>\n",
       "      <td id=\"T_be237_row1_col3\" class=\"data row1 col3\" >i was trying to explain to somebody as we were flying in that s corn </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be237_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_be237_row2_col0\" class=\"data row2 col0\" >2</td>\n",
       "      <td id=\"T_be237_row2_col1\" class=\"data row2 col1\" >str</td>\n",
       "      <td id=\"T_be237_row2_col2\" class=\"data row2 col2\" >3</td>\n",
       "      <td id=\"T_be237_row2_col3\" class=\"data row2 col3\" >that s beans </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be237_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_be237_row3_col0\" class=\"data row3 col0\" >3</td>\n",
       "      <td id=\"T_be237_row3_col1\" class=\"data row3 col1\" >str</td>\n",
       "      <td id=\"T_be237_row3_col2\" class=\"data row3 col2\" >9</td>\n",
       "      <td id=\"T_be237_row3_col3\" class=\"data row3 col3\" >and they were very impressed at my agricultural knowledge </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be237_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_be237_row4_col0\" class=\"data row4 col0\" >4</td>\n",
       "      <td id=\"T_be237_row4_col1\" class=\"data row4 col1\" >str</td>\n",
       "      <td id=\"T_be237_row4_col2\" class=\"data row4 col2\" >12</td>\n",
       "      <td id=\"T_be237_row4_col3\" class=\"data row4 col3\" >please give it up for amaury once again for that outstanding introduction </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be237_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_be237_row5_col0\" class=\"data row5 col0\" >5</td>\n",
       "      <td id=\"T_be237_row5_col1\" class=\"data row5 col1\" >str</td>\n",
       "      <td id=\"T_be237_row5_col2\" class=\"data row5 col2\" >38</td>\n",
       "      <td id=\"T_be237_row5_col3\" class=\"data row5 col3\" >i have a bunch of good friends here today including somebody who i served with who is one of the finest senators in the country and we re lucky to have him your senator dick durbin is here </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be237_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_be237_row6_col0\" class=\"data row6 col0\" >6</td>\n",
       "      <td id=\"T_be237_row6_col1\" class=\"data row6 col1\" >str</td>\n",
       "      <td id=\"T_be237_row6_col2\" class=\"data row6 col2\" >28</td>\n",
       "      <td id=\"T_be237_row6_col3\" class=\"data row6 col3\" >i also noticed by the way former governor edgar here who i haven t seen in a long time and somehow he has not aged and i have </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be237_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_be237_row7_col0\" class=\"data row7 col0\" >7</td>\n",
       "      <td id=\"T_be237_row7_col1\" class=\"data row7 col1\" >str</td>\n",
       "      <td id=\"T_be237_row7_col2\" class=\"data row7 col2\" >8</td>\n",
       "      <td id=\"T_be237_row7_col3\" class=\"data row7 col3\" >and it s great to see you governor </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be237_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_be237_row8_col0\" class=\"data row8 col0\" >8</td>\n",
       "      <td id=\"T_be237_row8_col1\" class=\"data row8 col1\" >str</td>\n",
       "      <td id=\"T_be237_row8_col2\" class=\"data row8 col2\" >24</td>\n",
       "      <td id=\"T_be237_row8_col3\" class=\"data row8 col3\" >i want to thank president killeen and everybody at the u of i system for making it possible for me to be here today </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be237_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_be237_row9_col0\" class=\"data row9 col0\" >9</td>\n",
       "      <td id=\"T_be237_row9_col1\" class=\"data row9 col1\" >str</td>\n",
       "      <td id=\"T_be237_row9_col2\" class=\"data row9 col2\" >16</td>\n",
       "      <td id=\"T_be237_row9_col3\" class=\"data row9 col3\" >and i am deeply honored at the paul douglas award that is being given to me </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be237_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_be237_row10_col0\" class=\"data row10 col0\" >10</td>\n",
       "      <td id=\"T_be237_row10_col1\" class=\"data row10 col1\" >str</td>\n",
       "      <td id=\"T_be237_row10_col2\" class=\"data row10 col2\" >16</td>\n",
       "      <td id=\"T_be237_row10_col3\" class=\"data row10 col3\" >he is somebody who set the path for so much outstanding public service here in illinois </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be237_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_be237_row11_col0\" class=\"data row11 col0\" >11</td>\n",
       "      <td id=\"T_be237_row11_col1\" class=\"data row11 col1\" >str</td>\n",
       "      <td id=\"T_be237_row11_col2\" class=\"data row11 col2\" >12</td>\n",
       "      <td id=\"T_be237_row11_col3\" class=\"data row11 col3\" >now i want to start by addressing the elephant in the room </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be237_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_be237_row12_col0\" class=\"data row12 col0\" >12</td>\n",
       "      <td id=\"T_be237_row12_col1\" class=\"data row12 col1\" >str</td>\n",
       "      <td id=\"T_be237_row12_col2\" class=\"data row12 col2\" >14</td>\n",
       "      <td id=\"T_be237_row12_col3\" class=\"data row12 col3\" >i know people are still wondering why i didn t speak at the commencement </td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f9659049820>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_data_frame(dataset)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #2 : Obtaining most frequent words in our text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the following steps to generate our model.\n",
    "\n",
    "We declare a dictionary to hold our bag of words.\n",
    "Next we tokenize each sentence to words.\n",
    "Now for each word in sentence, we check if the word exists in our dictionary.\n",
    "If it does, then we increment its count by 1. If it doesn’t, we add it to our dictionary and set its count as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' beans ',\n",
       " 'i was trying to explain to somebody as we were flying in that s corn ',\n",
       " 'that s beans ',\n",
       " 'and they were very impressed at my agricultural knowledge ',\n",
       " 'please give it up for amaury once again for that outstanding introduction ',\n",
       " 'i have a bunch of good friends here today including somebody who i served with who is one of the finest senators in the country and we re lucky to have him your senator dick durbin is here ',\n",
       " 'i also noticed by the way former governor edgar here who i haven t seen in a long time and somehow he has not aged and i have ',\n",
       " 'and it s great to see you governor ',\n",
       " 'i want to thank president killeen and everybody at the u of i system for making it possible for me to be here today ',\n",
       " 'and i am deeply honored at the paul douglas award that is being given to me ',\n",
       " 'he is somebody who set the path for so much outstanding public service here in illinois ',\n",
       " 'now i want to start by addressing the elephant in the room ',\n",
       " 'i know people are still wondering why i didn t speak at the commencement ']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model \n",
    "word2count = {} \n",
    "for data in dataset: \n",
    "\twords = nltk.word_tokenize(data) \n",
    "\tfor word in words: \n",
    "\t\tif word not in word2count.keys(): \n",
    "\t\t\tword2count[word] = 1\n",
    "\t\telse: \n",
    "\t\t\tword2count[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beans': 2,\n",
       " 'i': 12,\n",
       " 'was': 1,\n",
       " 'trying': 1,\n",
       " 'to': 8,\n",
       " 'explain': 1,\n",
       " 'somebody': 3,\n",
       " 'as': 1,\n",
       " 'we': 2,\n",
       " 'were': 2,\n",
       " 'flying': 1,\n",
       " 'in': 5,\n",
       " 'that': 4,\n",
       " 's': 3,\n",
       " 'corn': 1,\n",
       " 'and': 7,\n",
       " 'they': 1,\n",
       " 'very': 1,\n",
       " 'impressed': 1,\n",
       " 'at': 4,\n",
       " 'my': 1,\n",
       " 'agricultural': 1,\n",
       " 'knowledge': 1,\n",
       " 'please': 1,\n",
       " 'give': 1,\n",
       " 'it': 3,\n",
       " 'up': 1,\n",
       " 'for': 5,\n",
       " 'amaury': 1,\n",
       " 'once': 1,\n",
       " 'again': 1,\n",
       " 'outstanding': 2,\n",
       " 'introduction': 1,\n",
       " 'have': 3,\n",
       " 'a': 2,\n",
       " 'bunch': 1,\n",
       " 'of': 3,\n",
       " 'good': 1,\n",
       " 'friends': 1,\n",
       " 'here': 5,\n",
       " 'today': 2,\n",
       " 'including': 1,\n",
       " 'who': 4,\n",
       " 'served': 1,\n",
       " 'with': 1,\n",
       " 'is': 4,\n",
       " 'one': 1,\n",
       " 'the': 9,\n",
       " 'finest': 1,\n",
       " 'senators': 1,\n",
       " 'country': 1,\n",
       " 're': 1,\n",
       " 'lucky': 1,\n",
       " 'him': 1,\n",
       " 'your': 1,\n",
       " 'senator': 1,\n",
       " 'dick': 1,\n",
       " 'durbin': 1,\n",
       " 'also': 1,\n",
       " 'noticed': 1,\n",
       " 'by': 2,\n",
       " 'way': 1,\n",
       " 'former': 1,\n",
       " 'governor': 2,\n",
       " 'edgar': 1,\n",
       " 'haven': 1,\n",
       " 't': 2,\n",
       " 'seen': 1,\n",
       " 'long': 1,\n",
       " 'time': 1,\n",
       " 'somehow': 1,\n",
       " 'he': 2,\n",
       " 'has': 1,\n",
       " 'not': 1,\n",
       " 'aged': 1,\n",
       " 'great': 1,\n",
       " 'see': 1,\n",
       " 'you': 1,\n",
       " 'want': 2,\n",
       " 'thank': 1,\n",
       " 'president': 1,\n",
       " 'killeen': 1,\n",
       " 'everybody': 1,\n",
       " 'u': 1,\n",
       " 'system': 1,\n",
       " 'making': 1,\n",
       " 'possible': 1,\n",
       " 'me': 2,\n",
       " 'be': 1,\n",
       " 'am': 1,\n",
       " 'deeply': 1,\n",
       " 'honored': 1,\n",
       " 'paul': 1,\n",
       " 'douglas': 1,\n",
       " 'award': 1,\n",
       " 'being': 1,\n",
       " 'given': 1,\n",
       " 'set': 1,\n",
       " 'path': 1,\n",
       " 'so': 1,\n",
       " 'much': 1,\n",
       " 'public': 1,\n",
       " 'service': 1,\n",
       " 'illinois': 1,\n",
       " 'now': 1,\n",
       " 'start': 1,\n",
       " 'addressing': 1,\n",
       " 'elephant': 1,\n",
       " 'room': 1,\n",
       " 'know': 1,\n",
       " 'people': 1,\n",
       " 'are': 1,\n",
       " 'still': 1,\n",
       " 'wondering': 1,\n",
       " 'why': 1,\n",
       " 'didn': 1,\n",
       " 'speak': 1,\n",
       " 'commencement': 1}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import heapq \n",
    "# freq_words = heapq.nlargest(100, word2count, key=word2count.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    i  the  to  and  in  for  here  that  at  who  is  somebody  s  it  have  \\\n",
      "0   0    0   0    0   0    0     0     0   0    0   0         0  0   0     0   \n",
      "1   1    0   1    0   1    0     0     1   0    0   0         1  1   0     0   \n",
      "2   0    0   0    0   0    0     0     1   0    0   0         0  1   0     0   \n",
      "3   0    0   0    1   0    0     0     0   1    0   0         0  0   0     0   \n",
      "4   0    0   0    0   0    1     0     1   0    0   0         0  0   1     0   \n",
      "5   1    1   1    1   1    0     1     0   0    1   1         1  0   0     1   \n",
      "6   1    1   0    1   1    0     1     0   0    1   0         0  0   0     1   \n",
      "7   0    0   1    1   0    0     0     0   0    0   0         0  1   1     0   \n",
      "8   1    1   1    1   0    1     1     0   1    0   0         0  0   1     0   \n",
      "9   1    1   1    1   0    0     0     1   1    0   1         0  0   0     0   \n",
      "10  0    1   0    0   1    1     1     0   0    1   1         1  0   0     0   \n",
      "11  1    1   1    0   1    0     0     0   0    0   0         0  0   0     0   \n",
      "12  1    1   0    0   0    0     0     0   1    0   0         0  0   0     0   \n",
      "\n",
      "    of  beans  we  were  outstanding  a  today  by  governor  t  he  want  me  \\\n",
      "0    0      1   0     0            0  0      0   0         0  0   0     0   0   \n",
      "1    0      0   1     1            0  0      0   0         0  0   0     0   0   \n",
      "2    0      1   0     0            0  0      0   0         0  0   0     0   0   \n",
      "3    0      0   0     1            0  0      0   0         0  0   0     0   0   \n",
      "4    0      0   0     0            1  0      0   0         0  0   0     0   0   \n",
      "5    1      0   1     0            0  1      1   0         0  0   0     0   0   \n",
      "6    0      0   0     0            0  1      0   1         1  1   1     0   0   \n",
      "7    0      0   0     0            0  0      0   0         1  0   0     0   0   \n",
      "8    1      0   0     0            0  0      1   0         0  0   0     1   1   \n",
      "9    0      0   0     0            0  0      0   0         0  0   0     0   1   \n",
      "10   0      0   0     0            1  0      0   0         0  0   1     0   0   \n",
      "11   0      0   0     0            0  0      0   1         0  0   0     1   0   \n",
      "12   0      0   0     0            0  0      0   0         0  1   0     0   0   \n",
      "\n",
      "    was  trying  explain  as  flying  corn  they  very  impressed  my  \\\n",
      "0     0       0        0   0       0     0     0     0          0   0   \n",
      "1     1       1        1   1       1     1     0     0          0   0   \n",
      "2     0       0        0   0       0     0     0     0          0   0   \n",
      "3     0       0        0   0       0     0     1     1          1   1   \n",
      "4     0       0        0   0       0     0     0     0          0   0   \n",
      "5     0       0        0   0       0     0     0     0          0   0   \n",
      "6     0       0        0   0       0     0     0     0          0   0   \n",
      "7     0       0        0   0       0     0     0     0          0   0   \n",
      "8     0       0        0   0       0     0     0     0          0   0   \n",
      "9     0       0        0   0       0     0     0     0          0   0   \n",
      "10    0       0        0   0       0     0     0     0          0   0   \n",
      "11    0       0        0   0       0     0     0     0          0   0   \n",
      "12    0       0        0   0       0     0     0     0          0   0   \n",
      "\n",
      "    agricultural  knowledge  please  give  up  amaury  once  again  \\\n",
      "0              0          0       0     0   0       0     0      0   \n",
      "1              0          0       0     0   0       0     0      0   \n",
      "2              0          0       0     0   0       0     0      0   \n",
      "3              1          1       0     0   0       0     0      0   \n",
      "4              0          0       1     1   1       1     1      1   \n",
      "5              0          0       0     0   0       0     0      0   \n",
      "6              0          0       0     0   0       0     0      0   \n",
      "7              0          0       0     0   0       0     0      0   \n",
      "8              0          0       0     0   0       0     0      0   \n",
      "9              0          0       0     0   0       0     0      0   \n",
      "10             0          0       0     0   0       0     0      0   \n",
      "11             0          0       0     0   0       0     0      0   \n",
      "12             0          0       0     0   0       0     0      0   \n",
      "\n",
      "    introduction  bunch  good  friends  including  served  with  one  finest  \\\n",
      "0              0      0     0        0          0       0     0    0       0   \n",
      "1              0      0     0        0          0       0     0    0       0   \n",
      "2              0      0     0        0          0       0     0    0       0   \n",
      "3              0      0     0        0          0       0     0    0       0   \n",
      "4              1      0     0        0          0       0     0    0       0   \n",
      "5              0      1     1        1          1       1     1    1       1   \n",
      "6              0      0     0        0          0       0     0    0       0   \n",
      "7              0      0     0        0          0       0     0    0       0   \n",
      "8              0      0     0        0          0       0     0    0       0   \n",
      "9              0      0     0        0          0       0     0    0       0   \n",
      "10             0      0     0        0          0       0     0    0       0   \n",
      "11             0      0     0        0          0       0     0    0       0   \n",
      "12             0      0     0        0          0       0     0    0       0   \n",
      "\n",
      "    senators  country  re  lucky  him  your  senator  dick  durbin  also  \\\n",
      "0          0        0   0      0    0     0        0     0       0     0   \n",
      "1          0        0   0      0    0     0        0     0       0     0   \n",
      "2          0        0   0      0    0     0        0     0       0     0   \n",
      "3          0        0   0      0    0     0        0     0       0     0   \n",
      "4          0        0   0      0    0     0        0     0       0     0   \n",
      "5          1        1   1      1    1     1        1     1       1     0   \n",
      "6          0        0   0      0    0     0        0     0       0     1   \n",
      "7          0        0   0      0    0     0        0     0       0     0   \n",
      "8          0        0   0      0    0     0        0     0       0     0   \n",
      "9          0        0   0      0    0     0        0     0       0     0   \n",
      "10         0        0   0      0    0     0        0     0       0     0   \n",
      "11         0        0   0      0    0     0        0     0       0     0   \n",
      "12         0        0   0      0    0     0        0     0       0     0   \n",
      "\n",
      "    noticed  way  former  edgar  haven  seen  long  time  somehow  has  not  \\\n",
      "0         0    0       0      0      0     0     0     0        0    0    0   \n",
      "1         0    0       0      0      0     0     0     0        0    0    0   \n",
      "2         0    0       0      0      0     0     0     0        0    0    0   \n",
      "3         0    0       0      0      0     0     0     0        0    0    0   \n",
      "4         0    0       0      0      0     0     0     0        0    0    0   \n",
      "5         0    0       0      0      0     0     0     0        0    0    0   \n",
      "6         1    1       1      1      1     1     1     1        1    1    1   \n",
      "7         0    0       0      0      0     0     0     0        0    0    0   \n",
      "8         0    0       0      0      0     0     0     0        0    0    0   \n",
      "9         0    0       0      0      0     0     0     0        0    0    0   \n",
      "10        0    0       0      0      0     0     0     0        0    0    0   \n",
      "11        0    0       0      0      0     0     0     0        0    0    0   \n",
      "12        0    0       0      0      0     0     0     0        0    0    0   \n",
      "\n",
      "    aged  great  see  you  thank  president  killeen  everybody  u  system  \\\n",
      "0      0      0    0    0      0          0        0          0  0       0   \n",
      "1      0      0    0    0      0          0        0          0  0       0   \n",
      "2      0      0    0    0      0          0        0          0  0       0   \n",
      "3      0      0    0    0      0          0        0          0  0       0   \n",
      "4      0      0    0    0      0          0        0          0  0       0   \n",
      "5      0      0    0    0      0          0        0          0  0       0   \n",
      "6      1      0    0    0      0          0        0          0  0       0   \n",
      "7      0      1    1    1      0          0        0          0  0       0   \n",
      "8      0      0    0    0      1          1        1          1  1       1   \n",
      "9      0      0    0    0      0          0        0          0  0       0   \n",
      "10     0      0    0    0      0          0        0          0  0       0   \n",
      "11     0      0    0    0      0          0        0          0  0       0   \n",
      "12     0      0    0    0      0          0        0          0  0       0   \n",
      "\n",
      "    making  possible  be  am  deeply  honored  paul  douglas  award  being  \\\n",
      "0        0         0   0   0       0        0     0        0      0      0   \n",
      "1        0         0   0   0       0        0     0        0      0      0   \n",
      "2        0         0   0   0       0        0     0        0      0      0   \n",
      "3        0         0   0   0       0        0     0        0      0      0   \n",
      "4        0         0   0   0       0        0     0        0      0      0   \n",
      "5        0         0   0   0       0        0     0        0      0      0   \n",
      "6        0         0   0   0       0        0     0        0      0      0   \n",
      "7        0         0   0   0       0        0     0        0      0      0   \n",
      "8        1         1   1   0       0        0     0        0      0      0   \n",
      "9        0         0   0   1       1        1     1        1      1      1   \n",
      "10       0         0   0   0       0        0     0        0      0      0   \n",
      "11       0         0   0   0       0        0     0        0      0      0   \n",
      "12       0         0   0   0       0        0     0        0      0      0   \n",
      "\n",
      "    given  set  path  so  \n",
      "0       0    0     0   0  \n",
      "1       0    0     0   0  \n",
      "2       0    0     0   0  \n",
      "3       0    0     0   0  \n",
      "4       0    0     0   0  \n",
      "5       0    0     0   0  \n",
      "6       0    0     0   0  \n",
      "7       0    0     0   0  \n",
      "8       0    0     0   0  \n",
      "9       1    0     0   0  \n",
      "10      0    1     1   1  \n",
      "11      0    0     0   0  \n",
      "12      0    0     0   0  \n"
     ]
    }
   ],
   "source": [
    "X = [] \n",
    "for data in dataset: \n",
    "\tvector = [] \n",
    "\tfor word in freq_words: \n",
    "\t\tif word in nltk.word_tokenize(data): \n",
    "\t\t\tvector.append(1) \n",
    "\t\telse: \n",
    "\t\t\tvector.append(0) \n",
    "\tX.append(vector) \n",
    "X = np.asarray(X) \n",
    "\n",
    "# Create a DataFrame from the matrix for better visualization\n",
    "df = pd.DataFrame(X, columns=freq_words)\n",
    "\n",
    "# Display the entire DataFrame in a full-screen mode\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', None)     # Show all rows\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW) Model: Advantages and Disadvantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the data for advantages and disadvantages\n",
    "data = {\n",
    "    \"Aspect\": [\n",
    "        \"Simplicity\", \n",
    "        \"Efficiency\", \n",
    "        \"Feature Representation\", \n",
    "        \"Compatibility\", \n",
    "        \"Semantic Understanding\", \n",
    "        \"Vocabulary Handling\", \n",
    "        \"Model Training\", \n",
    "        \"Generalization\", \n",
    "        \"Use in NLP\"\n",
    "    ],\n",
    "    \"Advantages\": [\n",
    "        \"Easy to implement and understand.\",\n",
    "        \"Efficient for small datasets and basic tasks.\",\n",
    "        \"Provides a fixed-length vector representation.\",\n",
    "        \"Works well with traditional machine learning algorithms.\",\n",
    "        \"Does not require any semantic knowledge or language rules.\",\n",
    "        \"No need for handling complex linguistic variations.\",\n",
    "        \"Simplicity leads to faster model training in basic use cases.\",\n",
    "        \"Works for general text analysis tasks without the need for deep language understanding.\",\n",
    "        \"Suitable for tasks like spam detection and basic text classification.\"\n",
    "    ],\n",
    "    \"Disadvantages\": [\n",
    "        \"Ignores word order and context, losing semantic meaning.\",\n",
    "        \"High dimensionality for large vocabularies, leading to inefficiency.\",\n",
    "        \"Generates sparse matrices, which can be computationally expensive.\",\n",
    "        \"Limited to algorithms that can handle high-dimensional sparse data.\",\n",
    "        \"Cannot differentiate between synonyms or understand polysemy.\",\n",
    "        \"Cannot handle out-of-vocabulary (OOV) words, and treats variations of a word as separate features.\",\n",
    "        \"Prone to overfitting if the vocabulary size is too large or underfitting if it is too small.\",\n",
    "        \"Limited ability to generalize to new or unseen text due to fixed vocabulary.\",\n",
    "        \"Ineffective for complex NLP tasks that require contextual or sequential information.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Style the DataFrame without index\n",
    "styled_df = df.style.set_table_attributes('style=\"width:100%; border-collapse: collapse;\"') \\\n",
    "                    .set_properties(**{'border': '1px solid black'})\n",
    "\n",
    "# Display the styled DataFrame (without index)\n",
    "# styled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th>Aspect</th>\n",
       "      <th>Advantages</th>\n",
       "      <th>Disadvantages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Simplicity</td>\n",
       "      <td>Easy to implement and understand.</td>\n",
       "      <td>Ignores word order and context, losing semantic meaning.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Efficiency</td>\n",
       "      <td>Efficient for small datasets and basic tasks.</td>\n",
       "      <td>High dimensionality for large vocabularies, leading to inefficiency.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Feature Representation</td>\n",
       "      <td>Provides a fixed-length vector representation.</td>\n",
       "      <td>Generates sparse matrices, which can be computationally expensive.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Compatibility</td>\n",
       "      <td>Works well with traditional machine learning algorithms.</td>\n",
       "      <td>Limited to algorithms that can handle high-dimensional sparse data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Semantic Understanding</td>\n",
       "      <td>Does not require any semantic knowledge or language rules.</td>\n",
       "      <td>Cannot differentiate between synonyms or understand polysemy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Vocabulary Handling</td>\n",
       "      <td>No need for handling complex linguistic variations.</td>\n",
       "      <td>Cannot handle out-of-vocabulary (OOV) words, and treats variations of a word as separate features.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Model Training</td>\n",
       "      <td>Simplicity leads to faster model training in basic use cases.</td>\n",
       "      <td>Prone to overfitting if the vocabulary size is too large or underfitting if it is too small.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Generalization</td>\n",
       "      <td>Works for general text analysis tasks without the need for deep language understanding.</td>\n",
       "      <td>Limited ability to generalize to new or unseen text due to fixed vocabulary.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Use in NLP</td>\n",
       "      <td>Suitable for tasks like spam detection and basic text classification.</td>\n",
       "      <td>Ineffective for complex NLP tasks that require contextual or sequential information.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert to HTML without index\n",
    "html_output = df.to_html(index=False, border=0, justify='center')\n",
    "\n",
    "# Display the HTML output in a Jupyter notebook cell\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(html_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
