{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "One-Hot Encoding is a common method in Natural Language Processing (NLP) for representing text data as numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@kalyan45/natural-language-processing-one-hot-encoding-5b31f76b09a0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding is a common method in Natural Language Processing (NLP) for representing text data as numerical values. It transforms each word or token in a text dataset into a unique vector where only one element is \"hot\" (set to 1), and all others are \"cold\" (set to 0). This binary representation is especially useful in machine learning, where algorithms require numerical input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How One-Hot Encoding Works:\n",
    "Vocabulary Creation: First, a unique vocabulary list is created, consisting of all unique words (or tokens) in the dataset.\n",
    "Binary Vector Representation: Each word is assigned a binary vector. The length of the vector equals the total number of unique words in the vocabulary.\n",
    "Single Active Position: For each word, only one element in the vector is set to 1 (representing that word's index in the vocabulary), while all other elements are set to 0.\n",
    "Example\n",
    "Let's say we have a small vocabulary based on the sentence: \"I like NLP and NLP likes me.\"\n",
    "\n",
    "Vocabulary: [\"I\", \"like\", \"NLP\", \"and\", \"likes\", \"me\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of One-Hot Encoding\n",
    "Simple and easy to implement.\n",
    "Efficient for small vocabularies.\n",
    "### Disadvantages\n",
    "High Dimensionality: For large vocabularies, one-hot encoding creates very high-dimensional vectors, which is inefficient in terms of storage and computation.\n",
    "\n",
    "Sparse Vectors: Most elements in each vector are 0, leading to sparsity.\n",
    "\n",
    "Lack of Context: Words with similar meanings have completely different vectors, so thereâ€™s no information about word relationships or semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I love NLP. I love machine learning. NLP loves me\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['I love NLP', 'I love machine learning', 'NLP loves me']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re                               #The re module in Python is used here for regular expressions,\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Tokenize paragraph into sentences\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "# Remove punctuation from each sentence\n",
    "sentences = [re.sub(r'[^\\w\\s]', '', sentence) for sentence in sentences]\n",
    "\n",
    "print(\"Sentences:\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocess text by tokenizing and removing stop words (if necessary)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'love', 'nlp'],\n",
       " ['i', 'love', 'machine', 'learning'],\n",
       " ['nlp', 'loves', 'me']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning', 'love', 'loves', 'machine', 'nlp']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Step 2: Build vocabulary (unique words)\n",
    "vocabulary = sorted(set(word for sentence in tokenized_sentences for word in sentence if word not in stop_words))\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning': array([1., 0., 0., 0., 0.]),\n",
       " 'love': array([0., 1., 0., 0., 0.]),\n",
       " 'loves': array([0., 0., 1., 0., 0.]),\n",
       " 'machine': array([0., 0., 0., 1., 0.]),\n",
       " 'nlp': array([0., 0., 0., 0., 1.])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Step 3: Create one-hot encoding dictionary\n",
    "one_hot_dict = {word: np.eye(len(vocabulary))[i] for i, word in enumerate(vocabulary)}\n",
    "one_hot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0., 1., 0., 0., 0.]), array([0., 0., 0., 0., 1.])],\n",
       " [array([0., 1., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 1., 0.]),\n",
       "  array([1., 0., 0., 0., 0.])],\n",
       " [array([0., 0., 0., 0., 1.]), array([0., 0., 1., 0., 0.])]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Step 4: Encode sentences\n",
    "encoded_sentences = [[one_hot_dict[word] for word in sentence if word in one_hot_dict] for sentence in tokenized_sentences]\n",
    "encoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1: I love NLP\n",
      "  Word 'i' -> One-hot: [0. 1. 0. 0. 0.]\n",
      "  Word 'love' -> One-hot: [0. 0. 0. 0. 1.]\n",
      "\n",
      "Sentence 2: I love machine learning\n",
      "  Word 'i' -> One-hot: [0. 1. 0. 0. 0.]\n",
      "  Word 'love' -> One-hot: [0. 0. 0. 1. 0.]\n",
      "  Word 'machine' -> One-hot: [1. 0. 0. 0. 0.]\n",
      "\n",
      "Sentence 3: NLP loves me\n",
      "  Word 'nlp' -> One-hot: [0. 0. 0. 0. 1.]\n",
      "  Word 'loves' -> One-hot: [0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# # Display the one-hot encoding for each sentence\n",
    "for i, encoded_sentence in enumerate(encoded_sentences):\n",
    "    print(f\"\\nSentence {i+1}:\", sentences[i])\n",
    "    for j, vector in enumerate(encoded_sentence):\n",
    "        print(f\"  Word '{tokenized_sentences[i][j]}' -> One-hot: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Sample paragraph\n",
    "paragraph = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines computer science, \n",
    "artificial intelligence, and linguistics. NLP enables computers to understand, interpret, and respond to \n",
    "human language in a valuable way. With applications in various domains such as chatbots, translation, \n",
    "and sentiment analysis, NLP has transformed how we interact with technology.\"\"\"\n",
    "\n",
    "# Step 1: Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing (NLP) is a fascinating field that combines computer science, \\nartificial intelligence, and linguistics.',\n",
       " 'NLP enables computers to understand, interpret, and respond to \\nhuman language in a valuable way.',\n",
       " 'With applications in various domains such as chatbots, translation, \\nand sentiment analysis, NLP has transformed how we interact with technology.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Tokenize the paragraph into sentences\n",
    "tokenized_sentences = sent_tokenize(paragraph)\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Natural',\n",
       "  'Language',\n",
       "  'Processing',\n",
       "  '(',\n",
       "  'NLP',\n",
       "  ')',\n",
       "  'is',\n",
       "  'a',\n",
       "  'fascinating',\n",
       "  'field',\n",
       "  'that',\n",
       "  'combines',\n",
       "  'computer',\n",
       "  'science',\n",
       "  ',',\n",
       "  'artificial',\n",
       "  'intelligence',\n",
       "  ',',\n",
       "  'and',\n",
       "  'linguistics',\n",
       "  '.'],\n",
       " ['NLP',\n",
       "  'enables',\n",
       "  'computers',\n",
       "  'to',\n",
       "  'understand',\n",
       "  ',',\n",
       "  'interpret',\n",
       "  ',',\n",
       "  'and',\n",
       "  'respond',\n",
       "  'to',\n",
       "  'human',\n",
       "  'language',\n",
       "  'in',\n",
       "  'a',\n",
       "  'valuable',\n",
       "  'way',\n",
       "  '.'],\n",
       " ['With',\n",
       "  'applications',\n",
       "  'in',\n",
       "  'various',\n",
       "  'domains',\n",
       "  'such',\n",
       "  'as',\n",
       "  'chatbots',\n",
       "  ',',\n",
       "  'translation',\n",
       "  ',',\n",
       "  'and',\n",
       "  'sentiment',\n",
       "  'analysis',\n",
       "  ',',\n",
       "  'NLP',\n",
       "  'has',\n",
       "  'transformed',\n",
       "  'how',\n",
       "  'we',\n",
       "  'interact',\n",
       "  'with',\n",
       "  'technology',\n",
       "  '.']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Tokenize each sentence into words\n",
    "tokenized_words = [word_tokenize(sentence) for sentence in tokenized_sentences]\n",
    "tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'nlp',\n",
       "  'fascinating',\n",
       "  'field',\n",
       "  'combines',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'artificial',\n",
       "  'intelligence',\n",
       "  'linguistics'],\n",
       " ['nlp',\n",
       "  'enables',\n",
       "  'computers',\n",
       "  'understand',\n",
       "  'interpret',\n",
       "  'respond',\n",
       "  'human',\n",
       "  'language',\n",
       "  'valuable',\n",
       "  'way'],\n",
       " ['applications',\n",
       "  'various',\n",
       "  'domains',\n",
       "  'chatbots',\n",
       "  'translation',\n",
       "  'sentiment',\n",
       "  'analysis',\n",
       "  'nlp',\n",
       "  'transformed',\n",
       "  'interact',\n",
       "  'technology']]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Tokenize each sentence into words and remove stop words\n",
    "cleaned_sentences = [\n",
    "    [word for word in word_tokenize(sentence.lower()) if word.isalnum() and word not in stop_words]\n",
    "    for sentence in tokenized_sentences\n",
    "]\n",
    "cleaned_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening Process:\n",
    "\n",
    "The purpose of this line is to flatten the list of lists (cleaned_sentences) into a single list containing all the words.\n",
    "Without flattening, cleaned_sentences would be a list where each element is another list (i.e., a sentence).\n",
    "By flattening it, we extract each individual word from each sentence, resulting in a single list that contains all the words from all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'fascinating',\n",
       " 'field',\n",
       " 'combines',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'linguistics',\n",
       " 'nlp',\n",
       " 'enables',\n",
       " 'computers',\n",
       " 'understand',\n",
       " 'interpret',\n",
       " 'respond',\n",
       " 'human',\n",
       " 'language',\n",
       " 'valuable',\n",
       " 'way',\n",
       " 'applications',\n",
       " 'various',\n",
       " 'domains',\n",
       " 'chatbots',\n",
       " 'translation',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " 'nlp',\n",
       " 'transformed',\n",
       " 'interact',\n",
       " 'technology']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Step 4: Flatten the list of cleaned sentences to create a single list of words\n",
    "flattened_words = [word for sentence in cleaned_sentences for word in sentence]\n",
    "flattened_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlp',\n",
       " 'various',\n",
       " 'interact',\n",
       " 'field',\n",
       " 'translation',\n",
       " 'language',\n",
       " 'way',\n",
       " 'processing',\n",
       " 'valuable',\n",
       " 'transformed',\n",
       " 'combines',\n",
       " 'computer',\n",
       " 'technology',\n",
       " 'understand',\n",
       " 'natural',\n",
       " 'fascinating',\n",
       " 'respond',\n",
       " 'human',\n",
       " 'interpret',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'linguistics',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " 'chatbots',\n",
       " 'enables',\n",
       " 'domains',\n",
       " 'computers',\n",
       " 'science',\n",
       " 'applications']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Step 5: Create a unique vocabulary\n",
    "vocabulary = list(set(flattened_words))\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['nlp'],\n",
       "       ['various'],\n",
       "       ['interact'],\n",
       "       ['field'],\n",
       "       ['translation'],\n",
       "       ['language'],\n",
       "       ['way'],\n",
       "       ['processing'],\n",
       "       ['valuable'],\n",
       "       ['transformed'],\n",
       "       ['combines'],\n",
       "       ['computer'],\n",
       "       ['technology'],\n",
       "       ['understand'],\n",
       "       ['natural'],\n",
       "       ['fascinating'],\n",
       "       ['respond'],\n",
       "       ['human'],\n",
       "       ['interpret'],\n",
       "       ['artificial'],\n",
       "       ['intelligence'],\n",
       "       ['linguistics'],\n",
       "       ['sentiment'],\n",
       "       ['analysis'],\n",
       "       ['chatbots'],\n",
       "       ['enables'],\n",
       "       ['domains'],\n",
       "       ['computers'],\n",
       "       ['science'],\n",
       "       ['applications']], dtype='<U12')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # # Step 6: One-hot encoding\n",
    "# # Reshape the vocabulary for one-hot encoding\n",
    "vocabulary_array = np.array(vocabulary).reshape(-1, 1)\n",
    "vocabulary_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # # Fit and transform the vocabulary\n",
    "one_hot_encoded = encoder.fit_transform(vocabulary_array)\n",
    "one_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One-Hot Encoded Vectors:\n",
      "nlp: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "various: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0.]\n",
      "interact: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "field: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "translation: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "language: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "way: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1.]\n",
      "processing: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "valuable: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0.]\n",
      "transformed: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0.]\n",
      "combines: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "computer: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "technology: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "understand: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0.]\n",
      "natural: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "fascinating: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "respond: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "human: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "interpret: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "artificial: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "intelligence: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "linguistics: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "sentiment: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "analysis: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "chatbots: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "enables: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "domains: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "computers: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "science: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "applications: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Print one-hot encoded vectors\n",
    "print(\"\\nOne-Hot Encoded Vectors:\")\n",
    "for word, encoding in zip(vocabulary, one_hot_encoded):\n",
    "    print(f\"{word}: {encoding}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded vectors for the first sentence:\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the corpus of text\n",
    "corpus = [\n",
    "\t\"The quick brown fox jumped over the lazy dog.\",\n",
    "\t\"She sells seashells by the seashore.\",\n",
    "\t\"Peter Piper picked a peck of pickled peppers.\"\n",
    "]\n",
    "\n",
    "# Create a set of unique words in the corpus\n",
    "unique_words = set()\n",
    "for sentence in corpus:\n",
    "\tfor word in sentence.split():\n",
    "\t\tunique_words.add(word.lower())\n",
    "\n",
    "# Create a dictionary to map each\n",
    "# unique word to an index\n",
    "word_to_index = {}\n",
    "for i, word in enumerate(unique_words):\n",
    "\tword_to_index[word] = i\n",
    "\n",
    "# Create one-hot encoded vectors for\n",
    "# each word in the corpus\n",
    "one_hot_vectors = []\n",
    "for sentence in corpus:\n",
    "\tsentence_vectors = []\n",
    "\tfor word in sentence.split():\n",
    "\t\tvector = np.zeros(len(unique_words))\n",
    "\t\tvector[word_to_index[word.lower()]] = 1\n",
    "\t\tsentence_vectors.append(vector)\n",
    "\tone_hot_vectors.append(sentence_vectors)\n",
    "\n",
    "# Print the one-hot encoded vectors \n",
    "# for the first sentence\n",
    "print(\"One-hot encoded vectors for the first sentence:\")\n",
    "for vector in one_hot_vectors[0]:\n",
    "\tprint(vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences 1:\n",
      "The: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "cat: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "sat: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "on: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "the: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "mat.: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Sentences 2:\n",
      "The: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "dog: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "chased: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "the: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "cat.: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Sentences 3:\n",
      "The: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "mat: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "was: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "soft: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "and: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "fluffy.: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the sentences\n",
    "sentences = [\n",
    "\t'The cat sat on the mat.',\n",
    "\t'The dog chased the cat.',\n",
    "\t'The mat was soft and fluffy.'\n",
    "]\n",
    "\n",
    "# Create a vocabulary set\n",
    "vocab = set()\n",
    "for sentence in sentences:\n",
    "\twords = sentence.lower().split()\n",
    "\tfor word in words:\n",
    "\t\tvocab.add(word)\n",
    "\n",
    "# Create a dictionary to map words to integers\n",
    "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Create a binary vector for each word in each sentence\n",
    "vectors = []\n",
    "for sentence in sentences:\n",
    "\twords = sentence.lower().split()\n",
    "\tsentence_vectors = []\n",
    "\tfor word in words:\n",
    "\t\tbinary_vector = np.zeros(len(vocab))\n",
    "\t\tbinary_vector[word_to_int[word]] = 1\n",
    "\t\tsentence_vectors.append(binary_vector)\n",
    "\tvectors.append(sentence_vectors)\n",
    "\n",
    "# Print the one-hot encoded vectors for each word in each sentence\n",
    "for i in range(len(sentences)):\n",
    "\tprint(f\"Sentences {i + 1}:\")\n",
    "\tfor j in range(len(vectors[i])):\n",
    "\t\tprint(f\"{sentences[i].split()[j]}: {vectors[i][j]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Drawbacks of One-Hot Encoding in NLP : \n",
    "One of the major disadvantages of one-hot encoding in NLP is that it produces high-dimensional sparse vectors that can be extremely costly to process. This is due to the fact that one-hot encoding generates a distinct binary vector for each unique word in the text, resulting in a very big feature space. Furthermore, because one-hot encoding does not catch the semantic connections between words, machine-learning models that use these vectors as input may perform poorly. As a result, other encoding methods, such as word embeddings, are frequently used in NLP jobs. Word embeddings convert words into low-dimensional dense vectors that record meaningful connections between words, making them more useful for many NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
