{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f60bafc6-73fd-425a-846e-20bc41fe579f",
   "metadata": {},
   "source": [
    "### TF - IDF (Term Frequency-Inverse Document Frequency )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282f384a-8d30-46e5-9198-f7e917676cc7",
   "metadata": {},
   "source": [
    "#### ref : https://medium.com/@abhishekjainindore24/tf-idf-in-nlp-term-frequency-inverse-document-frequency-e05b65932f1d\n",
    "#### Ref 2 : https://www.markovml.com/blog/tf-idf\n",
    "#### Ref 3: https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
    "#### Ref 4 : https://www.kaggle.com/code/paulrohan2020/tf-idf-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bc7c51-9a38-4b95-9c14-36d1c8f8e452",
   "metadata": {},
   "source": [
    "#### Youtube ref: https://www.youtube.com/watch?v=ENLEjGozrio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764c2b01-784e-4995-9b0e-11497335f6a0",
   "metadata": {},
   "source": [
    "It's a statistical measure used in text mining and information retrieval to evaluate how important a word is to a document in a collection or corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965eaa95-9a5c-4f43-b52a-e13869b32670",
   "metadata": {},
   "source": [
    "Term Frequency (TF)\n",
    "Definition: The number of times a word appears in a document, divided by the total number of words in that document.\n",
    "Formula: TF = (Number of times the word appears in the document / Total number of words in the document)\n",
    "\n",
    "                        TF= (Total number of words in the document / Number of times the word appears in the document)\n",
    "\n",
    "Purpose: TF helps understand how frequently a word is used in a single document. Words that appear frequently in a document get a higher score.\n",
    "\n",
    "2. Inverse Document Frequency (IDF)\n",
    "Definition: A measure of how important a word is across a set of documents. Words that appear in many documents are less informative and thus given a lower weight.\n",
    "\n",
    "Formula:\n",
    "                    IDF = log( Total number of documents / Number of documents containing the word )\n",
    "\n",
    "\n",
    "Purpose: IDF helps reduce the importance of common words (like \"the\", \"is\", \"and\") that occur in many documents. Rare words that appear in fewer documents get a higher score.\n",
    "\n",
    "                                                        TF-IDF=TF×IDF\n",
    "\n",
    "Purpose: TF-IDF gives a weight to each word in a document, highlighting words that are important (i.e., frequent in a specific document but not common across all documents in the corpus).\n",
    "Example\n",
    "Let's say we have a corpus of 3 documents:\n",
    "\n",
    "\"The cat is on the mat.\"\n",
    "\"The dog is in the house.\"\n",
    "\"The cat and the dog are friends.\"\n",
    "Step 1: Compute Term Frequency (TF)\n",
    "\n",
    "For the word \"cat\" in Document 1:\n",
    "TF\n",
    "=\n",
    "1/6\n",
    "(\n",
    "since there are 6 words in total\n",
    ")\n",
    "TF= \n",
    "6\n",
    "1\n",
    "​\n",
    " (since there are 6 words in total)\n",
    "Step 2: Compute Inverse Document Frequency (IDF)\n",
    "\n",
    "\"cat\" appears in 2 out of 3 documents.\n",
    "IDF\n",
    "=\n",
    "log\n",
    "⁡\n",
    "(\n",
    "3\n",
    "2\n",
    ")\n",
    "IDF=log( \n",
    "2\n",
    "3\n",
    "​\n",
    " )\n",
    "Step 3: Compute TF-IDF\n",
    "\n",
    "TF-IDF\n",
    "=\n",
    "TF\n",
    "×\n",
    "IDF\n",
    "TF-IDF=TF×IDF\n",
    "Applications of TF-IDF\n",
    "Text Classification: It is used to convert text data into numerical features for machine learning models.\n",
    "Search Engines: Search engines use TF-IDF to rank documents based on their relevance to a search query.\n",
    "Document Similarity: Used in algorithms to find similar documents.\n",
    "Keyword Extraction: Helps in identifying important words in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b136493-743b-45cd-b7b1-cc45f8a134eb",
   "metadata": {},
   "source": [
    "## Applications of TF-IDF\n",
    "a) Text Classification: It is used to convert text data into numerical features for machine learning models.\n",
    "\n",
    "b) Search Engines: Search engines use TF-IDF to rank documents based on their relevance to a search query.\n",
    "\n",
    "c) Document Similarity: Used in algorithms to find similar documents.\n",
    "\n",
    "d) Keyword Extraction: Helps in identifying important words in a document(weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4d2bb87-6205-4858-b7e9-494b71bb4aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Define the advantages and disadvantages of TF-IDF\n",
    "# tfidf_info = {\n",
    "#     \"Advantages of TF-IDF\": [\n",
    "#         {\n",
    "#             \"Aspect\": \"Simplicity & Ease of Use\",\n",
    "#             \"Description\": \"TF-IDF is relatively easy to understand and implement, making it accessible for beginners and useful for quick prototyping.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"Aspect\": \"Effective for Basic Text Analysis\",\n",
    "#             \"Description\": \"It works well for text classification, document similarity, and keyword extraction in many scenarios, especially when you need a straightforward, interpretable representation of text.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"Aspect\": \"Highlights Important Words\",\n",
    "#             \"Description\": \"TF-IDF emphasizes unique and meaningful words by assigning higher scores to terms that are frequent in a document but rare across the corpus, which is useful for understanding key concepts.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"Aspect\": \"Reduces the Impact of Common Words\",\n",
    "#             \"Description\": \"By incorporating the inverse document frequency, it reduces the weight of common stop words (e.g., 'the', 'is', 'and'), making the representation more informative.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"Aspect\": \"Sparse Representation\",\n",
    "#             \"Description\": \"The output is usually a sparse matrix, which can be efficient in terms of storage and computation when used with libraries like SciPy.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"Aspect\": \"Widely Supported\",\n",
    "#             \"Description\": \"It is supported by many machine learning libraries and frameworks, like Scikit-Learn, which simplifies its application in projects.\"\n",
    "#         }\n",
    "#     ],\n",
    "#     \"Disadvantages of TF-IDF\": [\n",
    "#         {\n",
    "#             \"Aspect\": \"Ignores Word Order and Context\",\n",
    "#             \"Description\": \"TF-IDF treats text as a 'bag of words' and does not consider the order or context of words, which limits its ability to understand semantics. For example, 'New York' and 'York New' would be treated the same way.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"Aspect\": \"Not Suitable for Complex NLP Tasks\",\n",
    "#             \"Description\": \"For advanced natural language processing tasks like sentiment analysis, machine translation, or text generation, TF-IDF is insufficient as it does not capture meaning, context, or relationships between words.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"Aspect\": \"Vocabulary Size Can Be Large\",\n",
    "#             \"Description\": \"In large corpora, the number of unique words (features) can be very high, leading to increased memory consumption and slower performance.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"Aspect\": \"Suffers from High Dimensionality\",\n",
    "#             \"Description\": \"The sparse matrix representation can become inefficient for very large datasets, especially when working with millions of unique words.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"Aspect\": \"Static Weights\",\n",
    "#             \"Description\": \"The term weights in TF-IDF are static and do not adapt to changes in the document set. If the corpus is updated or expanded, the weights need to be recalculated, making it inefficient for dynamic or streaming data.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"Aspect\": \"Domain-Specific Challenges\",\n",
    "#             \"Description\": \"In some domains, important terms may not be frequent and can be underweighted by TF-IDF, which may require domain-specific adaptations.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"Aspect\": \"Lack of Handling for Synonyms and Polysemy\",\n",
    "#             \"Description\": \"TF-IDF does not understand that different words can mean the same thing (synonyms) or that the same word can have multiple meanings (polysemy). For example, 'car' and 'automobile' are treated as separate features even though they mean the same.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"Aspect\": \"No Inherent Handling of Out-of-Vocabulary Words\",\n",
    "#             \"Description\": \"When encountering new or rare words in test data that were not in the training corpus, TF-IDF has no built-in mechanism to handle them effectively.\"\n",
    "#         }\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# # Function to print the formatted advantages and disadvantages\n",
    "# def print_tfidf_info(tfidf_info):\n",
    "#     for category, items in tfidf_info.items():\n",
    "#         # print(f\"{category}\")\n",
    "#         for item in items:\n",
    "#             print(f\"\\n{item['Aspect']}\\n\")\n",
    "#             print(f\"{item['Description']}\\n\")\n",
    "\n",
    "# # Display the formatted information\n",
    "# # print_tfidf_info(tfidf_info)\n",
    "# # tfidf_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c7f2b-d82e-4ba7-8e34-12bb75b07bde",
   "metadata": {},
   "source": [
    "## Advantages of TF-IDF\n",
    "\n",
    "#### Simplicity & Ease of Use\n",
    "\n",
    "TF-IDF is relatively easy to understand and implement, making it accessible for beginners and useful for quick prototyping.\n",
    "\n",
    "#### Effective for Basic Text Analysis\n",
    "\n",
    "It works well for text classification, document similarity, and keyword extraction in many scenarios, especially when you need a straightforward, interpretable representation of text.\n",
    "\n",
    "#### Highlights Important Words\n",
    "\n",
    "TF-IDF emphasizes unique and meaningful words by assigning higher scores to terms that are frequent in a document but rare across the corpus, which is useful for understanding key concepts.\n",
    "\n",
    "#### Reduces the Impact of Common Words\n",
    "\n",
    "By incorporating the inverse document frequency, it reduces the weight of common stop words (e.g., \"the\", \"is\", \"and\"), making the representation more informative.\n",
    "\n",
    "#### Sparse Representation\n",
    "\n",
    "The output is usually a sparse matrix, which can be efficient in terms of storage and computation when used with libraries like SciPy.\n",
    "\n",
    "#### Widely Supported\n",
    "\n",
    "It is supported by many machine learning libraries and frameworks, like Scikit-Learn, which simplifies its application in projects.\n",
    "\n",
    "## Disadvantages of TF-IDF\n",
    "\n",
    "#### Ignores Word Order and Context\n",
    "\n",
    "TF-IDF treats text as a \"bag of words\" and does not consider the order or context of words, which limits its ability to understand semantics. For example, \"New York\" and \"York New\" would be treated the same way.\n",
    "\n",
    "#### Not Suitable for Complex NLP Tasks\n",
    "\n",
    "For advanced natural language processing tasks like sentiment analysis, machine translation, or text generation, TF-IDF is insufficient as it does not capture meaning, context, or relationships between words.\n",
    "\n",
    "#### Vocabulary Size Can Be Large\n",
    "\n",
    "In large corpora, the number of unique words (features) can be very high, leading to increased memory consumption and slower performance.\n",
    "\n",
    "#### Suffers from High Dimensionality\n",
    "\n",
    "The sparse matrix representation can become inefficient for very large datasets, especially when working with millions of unique words.\n",
    "\n",
    "#### Static Weights\n",
    "\n",
    "The term weights in TF-IDF are static and do not adapt to changes in the document set. If the corpus is updated or expanded, the weights need to be recalculated, making it inefficient for dynamic or streaming data.\n",
    "\n",
    "#### Domain-Specific Challenges\n",
    "\n",
    "In some domains, important terms may not be frequent and can be underweighted by TF-IDF, which may require domain-specific adaptations.\n",
    "\n",
    "#### Lack of Handling for Synonyms and Polysemy\n",
    "\n",
    "TF-IDF does not understand that different words can mean the same thing (synonyms) or that the same word can have multiple meanings (polysemy). For example, \"car\" and \"automobile\" are treated as separate features even though they mean the same.\n",
    "\n",
    "#### No Inherent Handling of Out-of-Vocabulary Words\n",
    "\n",
    "When encountering new or rare words in test data that were not in the training corpus, TF-IDF has no built-in mechanism to handle them effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faea3e4-d9bd-4146-a4e3-0a9ce5253186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
