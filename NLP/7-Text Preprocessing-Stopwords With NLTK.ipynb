{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All about Tokenization, Stop words, Stemming and Lemmatization in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link:  https://medium.com/@abhishekjainindore24/all-about-tokenization-stop-words-stemming-and-lemmatization-in-nlp-1620ffaf0f87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words:\n",
    "Definition: Stop words are common words (e.g., “the,” “is,” “and”) that are often removed during NLP tasks as they don’t carry significant meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "Original: “The quick brown fox jumps over the lazy dog.”\n",
    "After Removing Stop Words: [“quick”, “brown”, “fox”, “jumps”, “lazy”, “dog.”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing: Stopwords with NLTK\n",
    "Text Preprocessing is a crucial step in Natural Language Processing (NLP). It involves transforming raw text into a format that can be effectively used by machine learning models. One important task in text preprocessing is the removal of stopwords.\n",
    "\n",
    "What Are Stopwords?\n",
    "Stopwords are commonly used words in a language that carry very little information. These words are usually filtered out from text data because they don't contribute much to the analysis. Examples of stopwords in English include words like \"the\", \"is\", \"in\", \"and\", etc. Removing stopwords helps reduce noise and focuses the analysis on the more meaningful words.\n",
    "\n",
    "In the context of NLP, removing stopwords can help:\n",
    "\n",
    "Reduce dimensionality of the text data.\n",
    "Improve model performance by removing less informative words.\n",
    "Speed up computation by focusing only on important words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/koushikdev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/koushikdev/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words Applying on Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "# text = \"This is an example of text preprocessing with the Natural Language Toolkit.\"\n",
    "\n",
    "# # Tokenize the text into words\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# words = word_tokenize(text)\n",
    "# print(f\"words:\", words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the list of English stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# print(f\"stop_words:\", stop_words)\n",
    "# print(type(stop_words))  ## returns an set of stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Remove stopwords from the text\n",
    "# filter_words = [item for item in words if not item.lower() in stop_words]\n",
    "\n",
    "# print(\"Original_Words:\", words)\n",
    "# print(f\"filter_words:\", filter_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english'))\n",
    "# custom_stopwords = stop_words.union({'example', 'natural'})\n",
    "# # print(f\"custom_stopwords:\"), custom_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing certain words from stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words.remove('not') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speech Of DR APJ Abdul Kalam\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use PorterStemmer for  steamming \n",
    "# from nltk.stem import PorterStemmer\n",
    "# stemmer=PorterStemmer()\n",
    "# from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the paragraph into words\n",
    "# nltk.download('punkt')  # Ensure punkt tokenizer is available\n",
    "# words = word_tokenize(paragraph)\n",
    "# print(f\"words:\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting  paragraph to sentences using sent_tokenize\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# sentences = sent_tokenize(paragraph) \n",
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords And Filter And then Apply Stemming\n",
    "\n",
    "# for i in range(len(sentences)):\n",
    "#     words=nltk.word_tokenize(sentences[i])\n",
    "#     words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "#     sentences[i]=' '.join(words)# converting all the list of words into sentences\n",
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying Stemming on Words  using PorterStemmer\n",
    "# from nltk.stem import PorterStemmer\n",
    "# from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer=PorterStemmer()\n",
    "# snowballstemmer=SnowballStemmer('english') # using snowball stemmer for better accurecy \n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(paragraph)  # tokenize a paragraph in to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Organized Way\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# print(f\"stop_words:\", stop_words)\n",
    "\n",
    "def apply_stem_and_remove_stopwords(data):\n",
    "    # filter_stop_words = [snowballstemmer.stem(word) for word in data if word.lower() not in stop_words]\n",
    "    filter_stop_words = [lemmatizer.lemmatize(word.lower(),pos='v') for word in data if word not in stop_words]\n",
    "    return filter_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# words = word_tokenize(text)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i]) # converting each semtences in to  word tokenization \n",
    "    stem_words = apply_stem_and_remove_stopwords(words)\n",
    "    sentences[i]=' '.join(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three visions india .',\n",
       " '3000 years history , people world come invade us , capture land , conquer mind .',\n",
       " 'alexander onwards , greeks , turks , moguls , portuguese , british , french , dutch , come loot us , take .',\n",
       " 'yet nation .',\n",
       " 'conquer anyone .',\n",
       " 'grab land , culture , history try enforce way life .',\n",
       " '?',\n",
       " 'respect freedom others.that first vision freedom .',\n",
       " 'believe india get first vision 1857 , start war independence .',\n",
       " 'freedom must protect nurture build .',\n",
       " 'free , one respect us .',\n",
       " 'second vision india ’ development .',\n",
       " 'fifty years develop nation .',\n",
       " 'time see develop nation .',\n",
       " 'among top 5 nations world term gdp .',\n",
       " '10 percent growth rate areas .',\n",
       " 'poverty level fall .',\n",
       " 'achievements globally recognise today .',\n",
       " 'yet lack self-confidence see develop nation , self-reliant self-assured .',\n",
       " '’ incorrect ?',\n",
       " 'third vision .',\n",
       " 'india must stand world .',\n",
       " 'believe unless india stand world , one respect us .',\n",
       " 'strength respect strength .',\n",
       " 'must strong military power also economic power .',\n",
       " 'must go hand-in-hand .',\n",
       " 'good fortune work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear material .',\n",
       " 'lucky work three closely consider great opportunity life .',\n",
       " 'see four milestones career']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then Applying Lemmatization for more better accrucey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords And Filter And then Apply Snowball Stemming\n",
    "\n",
    "# for i in range(len(sentences)):\n",
    "#     words=nltk.word_tokenize(sentences[i])\n",
    "#     words=[snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "#     sentences[i]=' '.join(words)# converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import WordNetLemmatizer\n",
    "# lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Apply Stopwords And Filter And then Apply Snowball Stemming\n",
    "\n",
    "# for i in range(len(sentences)):\n",
    "#     #sentences[i]=sentences[i].lower()\n",
    "#     words=nltk.word_tokenize(sentences[i])\n",
    "#     words=[lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "#     sentences[i]=' '.join(words)# converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
